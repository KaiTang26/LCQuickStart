{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b48f486-6819-44ed-a325-a776c39aee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This's notebook for Langchang Quick Start\n"
     ]
    }
   ],
   "source": [
    "print(\"This's notebook for Langchang Quick Start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96c8ffd-3759-4d75-ae82-0bb2a78cdece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddc85eb0-2bd3-4616-86e4-643ea25b2795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models are powerful artificial intelligence models trained on vast amounts of text data that can generate human-like text and perform a wide range of natural language processing tasks.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "llm(\"explain large language models in one sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abc57fc7-1092-4924-9ab0-637d0cdc9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71505234-ac44-4242-b00a-7843aed200e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "message = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data\")\n",
    "]\n",
    "response = chat(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa5d02e8-c889-4c9b-bf5b-45f10ca73070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's an example of a Python script that trains a neural network on simulated data using the Keras library:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "\n",
      "# Generate simulated data\n",
      "np.random.seed(0)\n",
      "X = np.random.rand(1000, 10)\n",
      "y = np.random.randint(2, size=(1000, 1))\n",
      "\n",
      "# Define the neural network architecture\n",
      "model = Sequential()\n",
      "model.add(Dense(32, input_dim=10, activation='relu'))\n",
      "model.add(Dense(1, activation='sigmoid'))\n",
      "\n",
      "# Compile the model\n",
      "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "\n",
      "# Train the model\n",
      "model.fit(X, y, epochs=10, batch_size=32)\n",
      "\n",
      "# Evaluate the model\n",
      "loss, accuracy = model.evaluate(X, y)\n",
      "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
      "```\n",
      "\n",
      "In this script, we first generate simulated data using `numpy`. We create an array `X` of shape (1000, 10) with random values between 0 and 1, and an array `y` of shape (1000, 1) with random binary labels (0 or 1).\n",
      "\n",
      "Then, we define the neural network architecture using `Sequential` from Keras. We add a fully connected layer with 32 units and ReLU activation as the input layer, and a single unit with sigmoid activation as the output layer.\n",
      "\n",
      "Next, we compile the model using binary cross-entropy loss and the Adam optimizer.\n",
      "\n",
      "We then train the model using the `fit` method, specifying the input data `X` and labels `y`, the number of epochs, and the batch size.\n",
      "\n",
      "Finally, we evaluate the trained model on the same data using the `evaluate` method and print the loss and accuracy.\n"
     ]
    }
   ],
   "source": [
    "print(response.content, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f01c203-570c-4e45-83a2-27327b9f2525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models.\n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9ee18cd-8944-4af5-8988-ed980a34642e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], output_parser=None, partial_variables={}, template='\\nYou are an expert data scientist with an expertise in building deep learning models.\\nExplain the concept of {concept} in a couple of lines\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bac2ddc-2b71-4289-b080-a368896f6dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An autoencoder is a type of neural network that learns to compress and then reconstruct input data. It consists of an encoder that compresses the data into a latent representation and a decoder that reconstructs the original input. Autoencoders are primarily used for dimensionality reduction, feature extraction, and anomaly detection tasks.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt.format(concept=\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a4b09-8bbd-4df8-9dce-a5cdcfe5b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
